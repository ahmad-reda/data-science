**Concepts covered in the Machine Learning course**
Unfortunately, the source code of the projects can not be provided according to the honor of code of Edx.

-- Unit 1:
- Linear classifiers
- Linear separation
- Perceptron algorithm
- Regularization
- Stochastic gradient descent
- Generalization
- Passive-aggressive algorithm

-- Project 1- Automatic Review Analyzer:
- Pegasos algorithm
- Average perceptron algorithm
- Feature vectors
- Bag of words
- Unigrams and Bigrams
- Parameter tuning
- Feature engineering

-- Unit 2:
- Closed form solution of Linear regression
- Regularization and Generalization in Linear regression
- Non-linear classification using Feature maps and Feature vectors transformation
- Kernel functions, Kernel perceptron, Kernel linear regression
- Recommender system using K-Nearest Neighbor method
- Collaborative filtering with low rank assumption

-- Project 2- Digit recognition (Part 1):
- Working on MNIST database
- Linear regression
- One vs. Rest SVM
- Multiclass SVM
- Softmax regression
- Dimensionality Reduction via PCA
- Cubic Features
- Kernlizing the softmax regression

-- Unit 3:
- Neural networks layers and weights 
- Activation functions
- Linear separation using hidden layers
- Back-propagation using recursive relations
- Training using stochastic gradient descent algorithm
- Recurrent neural networks (RNNs)
- Gating and memory cells in LSTM
- Encoding sequences using RNNs
- The relation between RNNs and Markov models
- Decoding using RNNs to generate sequences
- Word Embeddings
- Convolutional neural networks (CNNs)
- Convolution and max pooling units

-- Project 3- Digit recognition (Part 2):
- Working on MNIST database
- Neural networks from scratch
- Predictions of the test set
- Deep neural networks using PyTorch
- Hyperparameter  tuning of a fully connected network
- Convolutional neural network with flatten layer
- Regularization using dropout layers
- Multi-overlapped-digit recognition

-- Unit 4:
- Clustering and image quantization
- Similarity Measures as clustering cost
- K-means algorithm
- K-Medoids algorithm
- Number of clusters choice
- Supervised elements in the unsupervised learning
- Generative Models
- Maximum Likelihood Estimates (MLE) for multinomial and Gaussian generative models
- EM (Expectation Maximization) algorithm for mixture models

-- Project 4- Collaborative Filtering via Gaussian Mixtures:
- Working on Netflix movie database
- Log-likelihood
- Bayesian Information Criterion
- Mixture models for matrix completion

-- Unit 5:
- Markov decision process and transition Probabilities
- Finite horizon-based utility
- Discounted reward-based utility
- Policy and Value Functions
- Bellman equations for Q-function
- Value Iteration and Q-value iteration
- Model free vs model-based approaches
- Q-value iteration by sampling
- Exploration vs Exploitation using Epsilon-Greedy approach


-- Project 5- Text-Based Game:
- Rewards definition
- Tabular Q-learning
- Parameter tuning for the Epsilon-Greedy approach and the learning rate
- Q-learning with linear function approximation
- Deep Q-network